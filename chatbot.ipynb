{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "846f93ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ [í˜ì´ì§€ 1] í¬ë¡¤ë§ ì¤‘...\n",
      "ğŸ” ì²˜ë¦¬ ì¤‘: [ëª¨ì§‘ê³µê³ ] ã€Œ2025 ì„¸ê³„ì¸ê¶Œë„ì‹œí¬ëŸ¼ã€ì•ˆë‚´\n",
      "ğŸ” ì²˜ë¦¬ ì¤‘: [ì¥í•™ì•ˆë‚´] 2025í•™ë…„ë„ ì—¬ìˆ˜ìº í¼ìŠ¤ í•™ìƒì„±ê³µì§€ì›ê¸ˆ ì„ ë°œ ê³„íš ì•ˆë‚´\n",
      "ğŸ” ì²˜ë¦¬ ì¤‘: [í•™ì‚¬ì•ˆë‚´] 2024í•™ë…„ë„ í›„ê¸°(2025ë…„ 8ì›”) í•™ë¶€ ì¡¸ì—…(ìˆ˜ë£Œ) ì˜ˆë¹„ì‚¬...\n",
      "ğŸ” ì²˜ë¦¬ ì¤‘: [ëŒ€í•™ìƒí™œ] 2025ë…„ã€ŒCNUë‚˜ëˆ”-ì „ê³µì—°ê³„ êµ­ë‚´ë´‰ì‚¬ã€ì°¸ì—¬íŒ€ ëª¨ì§‘\n",
      "ğŸ” ì²˜ë¦¬ ì¤‘: [ëŒ€í•™ìƒí™œ] êµ­ì œí•™ìƒì¦ ISIC ì²´í¬ì¹´ë“œ ë¬´ë£Œ ë°œê¸‰ í–‰ì‚¬ ì•ˆë‚´\n",
      "ğŸ” ì²˜ë¦¬ ì¤‘: [í•™ì‚¬ì•ˆë‚´] 2025í•™ë…„ë„ í•˜ê³„ ê³„ì ˆí•™ê¸° ìš´ì˜ ê³„íš ì•ˆë‚´\n",
      "ğŸ” ì²˜ë¦¬ ì¤‘: [ëŒ€í•™ìƒí™œ] ìš©ë´‰ìº í¼ìŠ¤ êµë‚´ì‹ë‹¹(ì œ1í•™ìƒë§ˆë£¨ í•™ìƒì‹ë‹¹, í–‡ë“¤ë§ˆë£¨) ì‹ë‹¨ê°€ ...\n",
      "ğŸ” ì²˜ë¦¬ ì¤‘: [í•™ì‚¬ì•ˆë‚´] 2024í•™ë…„ë„ í›„ê¸°(2025ë…„ 8ì›”) í•™ë¶€ ì¡¸ì—…(ìˆ˜ë£Œ)ëŒ€ìƒì ...\n",
      "ğŸ” ì²˜ë¦¬ ì¤‘: [ëŒ€í•™ìƒí™œ] 2025í•™ë…„ë„ 1í•™ê¸° 'ì²œì›ì˜ ì•„ì¹¨ë°¥' ì œê³µ...\n",
      "ğŸ” ì²˜ë¦¬ ì¤‘: [í•™ì‚¬ì•ˆë‚´] 2025í•™ë…„ë„ ì‹ (í¸)ì…ìƒ í•™ìƒì¦(ìŠ¤ë§ˆíŠ¸ì¹´ë“œ) ë°œê¸‰ ì‹ ì²­ ì•ˆë‚´\n",
      "ğŸ” ì²˜ë¦¬ ì¤‘: [ëŒ€í•™ìƒí™œ] 2025í•™ë…„ë„ ëŒ€í•™ìƒí™œì•ˆë‚´\n",
      "ğŸ” ì²˜ë¦¬ ì¤‘: [í•™ì‚¬ì•ˆë‚´] 2025í•™ë…„ë„ 1í•™ê¸° êµì–‘êµê³¼ëª© í¸ì„± ëª©ë¡ ì•ˆë‚´\n",
      "ğŸ” ì²˜ë¦¬ ì¤‘: [í•™ì‚¬ì•ˆë‚´] 2025í•™ë…„ë„ ì œ1í•™ê¸° íœ´í•™Â·ë³µí•™ ì‹ ì²­ ì•ˆë‚´\n",
      "ğŸ” ì²˜ë¦¬ ì¤‘: [ëª¨ì§‘ê³µê³ ] 2025-2026í•™ë…„ë„ ì „ë‚¨ëŒ€ í•´ì™¸ íŒŒê²¬í”„ë¡œê·¸ë¨ ì„ ë°œ ì¼ì •\n",
      "ğŸ” ì²˜ë¦¬ ì¤‘: [ëª¨ì§‘ê³µê³ ] ã€2025 ëŒ€í•œë¯¼êµ­ ì—´ë¦° í† ë¡ ëŒ€íšŒã€ ë…¼ì œ ê³µëª¨\n",
      "ğŸ” ì²˜ë¦¬ ì¤‘: [ê³µëª¨ì „] ê¸°ìˆ ë³´ì¦ê¸°ê¸ˆ ã€Œ2025ë…„ë„ ëŒ€êµ­ë¯¼ í˜ì‹  ì•„ì´ë””ì–´ ê³µëª¨ì „ã€ì•ˆë‚´\n",
      "ğŸ” ì²˜ë¦¬ ì¤‘: [ê³µëª¨ì „] ã€Œ2025 ë‚ ì”¨ ë¹…ë°ì´í„° ì½˜í…ŒìŠ¤íŠ¸ã€ ê°œìµœ ì•Œë¦¼\n",
      "ğŸ” ì²˜ë¦¬ ì¤‘: [ê³µëª¨ì „] 2025ë…„ IP ì•„ì¹´ë°ë¯¸ ì¹´ë“œë‰´ìŠ¤ ê³µëª¨ì „ ì•ˆë‚´\n",
      "ğŸ” ì²˜ë¦¬ ì¤‘: [ê³µëª¨ì „] ã€Œ2025ë…„ ëŒ€êµ­ë¯¼ ë†ì‹í’ˆ ê·œì œí˜ì‹  ê³µëª¨ì „ã€ê°œìµœ ì•Œë¦¼\n",
      "ğŸ” ì²˜ë¦¬ ì¤‘: [ì·¨ì—…ì •ë³´] [ë†ì—…ì •ì±…ë³´í—˜ê¸ˆìœµì›] 2025 ë†ì—…ì •ì±…ë³´í—˜ê¸ˆìœµì› ì œ3ì°¨ ì§ì› ...\n",
      "\n",
      "âœ… ì´ 20ê°œì˜ ê³µì§€ ìˆ˜ì§‘ ì™„ë£Œ!\n",
      "\n",
      "[ëª¨ì§‘ê³µê³ ] ã€Œ2025 ì„¸ê³„ì¸ê¶Œë„ì‹œí¬ëŸ¼ã€ì•ˆë‚´\n",
      "ğŸ”— https://www.jnu.ac.kr/WebApp/web/HOM/COM/Board/board.aspx?boardID=5&bbsMode=view&page=1&key=65952\n",
      "ã€Œ2025 ì„¸ê³„ì¸ê¶Œë„ì‹œí¬ëŸ¼ã€ì•ˆë‚´  ê´‘ì£¼ê´‘ì—­ì‹œëŠ” 2011ë…„ë¶€í„° â€˜ì¸ê¶Œë„ì‹œ ê´‘ì£¼â€™ì˜ ë¹„ì „ì„ ì‹¤í˜„í•˜ê¸° ìœ„í•´ ë§¤ë…„ ì„¸ê³„ì¸ê¶Œë„ì‹œí¬ëŸ¼ì„ ê°œìµœí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì™€ ê°™ì´ ã€Œ2025 ì„¸ê³„ì¸ê¶Œë„ì‹œ... \n",
      "\n",
      "[ì¥í•™ì•ˆë‚´] 2025í•™ë…„ë„ ì—¬ìˆ˜ìº í¼ìŠ¤ í•™ìƒì„±ê³µì§€ì›ê¸ˆ ì„ ë°œ ê³„íš ì•ˆë‚´\n",
      "ğŸ”— https://www.jnu.ac.kr/WebApp/web/HOM/COM/Board/board.aspx?boardID=5&bbsMode=view&page=1&key=65906\n",
      "2025í•™ë…„ë„ ì—¬ìˆ˜ìº í¼ìŠ¤ í•™ìƒì„±ê³µì§€ì›ê¸ˆ ì„ ë°œ ê³„íšì„ ë‹¤ìŒê³¼ ê°™ì´ ì•ˆë‚´í•˜ì˜¤ë‹ˆ ë§ì€ ì§€ì› ë°”ë¼ë©°, ì§€ì›ì„œ ì ‘ìˆ˜ ì „ ë¶™ì„ ë¬¸ì„œë¥¼ ë°˜ë“œì‹œ ìˆ™ì§€í•˜ì‹  í›„ ì „ë‚¨ëŒ€í•™êµ í¬í„¸ì‚¬ì´íŠ¸ë¥¼ í†µí•˜ì—¬ ì˜¨ë¼ì¸... \n",
      "\n",
      "[í•™ì‚¬ì•ˆë‚´] 2024í•™ë…„ë„ í›„ê¸°(2025ë…„ 8ì›”) í•™ë¶€ ì¡¸ì—…(ìˆ˜ë£Œ) ì˜ˆë¹„ì‚¬...\n",
      "ğŸ”— https://www.jnu.ac.kr/WebApp/web/HOM/COM/Board/board.aspx?boardID=5&bbsMode=view&page=1&key=65905\n",
      "â–¡ 2024í•™ë…„ë„ í›„ê¸°(2025ë…„ 8ì›”) í•™ë¶€ ì¡¸ì—…(ìˆ˜ë£Œ) ì˜ˆë¹„ì‚¬ì • ì‹¤ì‹œ ì•ˆë‚´ â–¡  2024í•™ë…„ë„ í›„ê¸°(2025ë…„ 8ì›”) í•™ë¶€ ì¡¸ì—…(ìˆ˜ë£Œ) ì˜ˆë¹„ì‚¬ì •ì„ ë‹¤ìŒê³¼ ê°™ì´ ì‹¤ì‹œí•˜ë‹ˆ, í•´ë‹¹ ... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ì „ë‚¨ëŒ€í•™êµ ê³µì§€ì‚¬í•­ í¬ë¡¤ëŸ¬\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import os\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = '/opt/homebrew/bin/tesseract'\n",
    "os.environ['TESSDATA_PREFIX'] = '/opt/homebrew/share/tessdata'\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\") \n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# ìœ íš¨í•œ ì¹´í…Œê³ ë¦¬ ì •ì˜\n",
    "valid_categories = [\"í•™ì‚¬ì•ˆë‚´\", \"ëŒ€í•™ìƒí™œ\", \"ëª¨ì§‘ê³µê³ \", \"ê³µëª¨ì „\", \"ì±„ìš©ê³µê³ \", \"ì·¨ì—…ì •ë³´\", \"ì¥í•™ì•ˆë‚´\", \"ë³‘ë¬´ì•ˆë‚´\"]\n",
    "\n",
    "# í¬ë¡¤ë§ëœ ë°ì´í„° ì €ì¥ ë¦¬ìŠ¤íŠ¸\n",
    "data = []\n",
    "\n",
    "# í¬í„¸ ê¸°ë³¸ ì£¼ì†Œ\n",
    "base_url = \"https://www.jnu.ac.kr\"\n",
    "\n",
    "# í˜ì´ì§€ ë²”ìœ„ ì„¤ì •\n",
    "for page in range(1, 2):\n",
    "    print(f\"ğŸ“„ [í˜ì´ì§€ {page}] í¬ë¡¤ë§ ì¤‘...\")\n",
    "    url = f\"https://www.jnu.ac.kr/WebApp/web/HOM/COM/Board/board.aspx?boardID=5&bbsMode=list&cate=0&page={page}\"\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # ì „ë‚¨ëŒ€ í™ˆí˜ì´ì§€ ê³µì§€ì‚¬í•­ì—ì„œ ì œëª© íƒœê·¸\n",
    "    notice_elements = driver.find_elements(By.CSS_SELECTOR, \"td.title > a\")\n",
    "    \n",
    "    # í™ˆí˜ì´ì§€ ì œëª©ìˆ˜ì§‘\n",
    "    notices_to_process = []\n",
    "    for el in notice_elements:\n",
    "        try:\n",
    "            full_title = el.text.strip()\n",
    "            relative_link = el.get_attribute(\"href\")\n",
    "            link = relative_link \n",
    "            match = re.match(r\"\\[(.*?)\\]\\s*(.*)\", full_title)\n",
    "            # ì¹´í…Œê³ ë¦¬ ë³„ ë¶„ë¥˜\n",
    "            if match:\n",
    "                category = match.group(1)\n",
    "                title = match.group(2)\n",
    "            else:\n",
    "                category = \"ê¸°íƒ€\"\n",
    "                title = full_title\n",
    "                \n",
    "            if category not in valid_categories:\n",
    "                category = \"ê¸°íƒ€\"\n",
    "                \n",
    "            notices_to_process.append({\n",
    "                \"category\": category,\n",
    "                \"title\": title,\n",
    "                \"link\": link\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ì œëª©/ë§í¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    # ìˆ˜ì§‘ëœ ë§í¬ë¥¼ í•˜ë‚˜ì”© ë°©ë¬¸í•˜ì—¬ ë‚´ìš© í¬ë¡¤ë§\n",
    "    for notice in notices_to_process:\n",
    "        try:\n",
    "            print(f\"ğŸ” ì²˜ë¦¬ ì¤‘: [{notice['category']}] {notice['title']}\")\n",
    "            driver.get(notice['link'])\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ìˆ˜ì§‘\n",
    "            try:\n",
    "                text_content = driver.find_element(By.CLASS_NAME, \"view_body\").text.strip()\n",
    "                text_content = text_content.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "                text_content = \"\"\n",
    "            \n",
    "            # ì´ë¯¸ì§€ OCR í…ìŠ¤íŠ¸ ìˆ˜ì§‘\n",
    "            ocr_text = \"\"\n",
    "            try:\n",
    "                images = driver.find_elements(By.CSS_SELECTOR, \".view_body img\")\n",
    "                for img in images:\n",
    "                    img_url = img.get_attribute(\"src\")\n",
    "                    if not img_url.startswith(\"http\"):\n",
    "                        img_url = base_url + img_url\n",
    "                    \n",
    "                    try:\n",
    "                        response = requests.get(img_url)\n",
    "                        image = Image.open(BytesIO(response.content))\n",
    "                        \n",
    "                        ocr_text += pytesseract.image_to_string(image, lang=\"kor\") + \"\\n\"\n",
    "                    except Exception as e:\n",
    "                        print(f\"âš ï¸ ì´ë¯¸ì§€ OCR ì‹¤íŒ¨: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ ì´ë¯¸ì§€ ìš”ì†Œ ì°¾ê¸° ì‹¤íŒ¨: {e}\")\n",
    "            \n",
    "            # ìˆ˜ì§‘í•œ ë‚´ìš© ì €ì¥\n",
    "            content = text_content + \"\\n\" + ocr_text.strip()\n",
    "            notice[\"content\"] = content\n",
    "            data.append(notice)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ ê³µì§€ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}\")\n",
    "driver.quit()\n",
    "\n",
    "# âœ… ê²°ê³¼ í™•ì¸ \n",
    "print(f\"\\nâœ… ì´ {len(data)}ê°œì˜ ê³µì§€ ìˆ˜ì§‘ ì™„ë£Œ!\\n\")\n",
    "for notice in data[:3]:\n",
    "    print(f\"[{notice['category']}] {notice['title']}\")\n",
    "    print(f\"ğŸ”— {notice['link']}\")\n",
    "    print(notice['content'][:100] + \"...\" if notice['content'] else \"ë‚´ìš© ì—†ìŒ\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d09f29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# langchainì„ í™œìš©í•˜ì—¬ ë°ì´í„° ì²­í‚¹ \n",
    "def chunk_documents(data, chunk_size=150, chunk_overlap=25):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    chunked_documents = []\n",
    "\n",
    "    for item in data:\n",
    "        metadata = {\n",
    "            \"category\": item.get(\"category\", \"\"),\n",
    "            \"title\": item.get(\"title\", \"\"),\n",
    "            \"link\": item.get(\"link\", \"\"),\n",
    "            \"source_id\": item.get(\"id\", \"\") if \"id\" in item else f\"{item.get('title', '')[:20]}\"\n",
    "        }\n",
    "\n",
    "        title = item.get(\"title\", \"\")\n",
    "        category = item.get(\"category\", \"\")\n",
    "        content = item.get(\"content\", \"\")\n",
    "        link = item.get(\"link\", \"\")\n",
    "\n",
    "        if content:\n",
    "            content_chunks = text_splitter.split_text(content)\n",
    "\n",
    "            for i, chunk in enumerate(content_chunks):\n",
    "                # ë°©ì‹ 1ë²ˆ : full_chunk = chunk\n",
    "                # ë°©ì‹ 2ë²ˆ : category + title + chunk ì¡°í•©\n",
    "                full_chunk = f\"[{category}] {title} - {chunk}\"\n",
    "                # full_chunk += f\" (ì¶œì²˜: {link})\"\n",
    "                \n",
    "                doc = Document(\n",
    "                    page_content=full_chunk,\n",
    "                    metadata={\n",
    "                        **metadata,\n",
    "                        \"chunk_id\": i,\n",
    "                        \"chunk_count\": len(content_chunks)\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                chunked_documents.append(doc)\n",
    "        else:\n",
    "            full_chunk = f\"[{category}] {title}\"\n",
    "            doc = Document(\n",
    "                page_content=full_chunk,\n",
    "                metadata=metadata\n",
    "            )\n",
    "            chunked_documents.append(doc)\n",
    "\n",
    "    return chunked_documents\n",
    "\n",
    "\n",
    "\n",
    "# í•œêµ­ì–´ ì „ìš© SBERT ëª¨ë¸ ë¡œë”©\n",
    "embedding_model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')\n",
    "\n",
    "# ì˜ˆì‹œ ì„ë² ë”© í•¨ìˆ˜\n",
    "def embed_documents(docs):\n",
    "    texts = [doc.page_content for doc in docs]\n",
    "    embeddings = embedding_model.encode(texts, show_progress_bar=True)\n",
    "    return embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2e464e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:12<00:00,  1.12s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.84016085, -0.01643609, -0.23869938, ..., -0.4581829 ,\n",
       "        -0.19099139,  0.24905178],\n",
       "       [-1.0638907 , -0.20040065, -0.00766017, ...,  0.07699399,\n",
       "         0.26231852,  0.03336357],\n",
       "       [-0.8535307 , -0.54984456,  0.20781986, ..., -0.05521315,\n",
       "        -0.30920595, -0.07586239],\n",
       "       ...,\n",
       "       [-0.4667073 , -0.37870374, -0.7335357 , ...,  0.571243  ,\n",
       "         0.49001592, -0.20996752],\n",
       "       [ 0.24263811, -0.2938347 , -0.6003033 , ...,  0.45064425,\n",
       "         0.6354364 , -0.45687437],\n",
       "       [-1.0110147 , -0.17097613, -0.94552153, ...,  0.2176166 ,\n",
       "         0.18630247, -0.05454658]], shape=(346, 768), dtype=float32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_docs = chunk_documents(data)\n",
    "embed_documents(chunked_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62a855f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë¬¸ì„œ ë‚´ë¶€ ìœ ì‚¬ë„ (intra-document avg)\n",
      "ë°©ì‹1 í‰ê· : 0.7137\n",
      "\n",
      "âœ… ë¬¸ì„œ ê°„ ìœ ì‚¬ë„ (inter-document avg)\n",
      "ë°©ì‹1 í‰ê· : 0.5358\n"
     ]
    }
   ],
   "source": [
    "# ìœ ì‚¬ë„ ê³„ì‚°í•˜ê¸°\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def group_by_source(docs):\n",
    "    groups = defaultdict(list)\n",
    "    for i, doc in enumerate(docs):\n",
    "        groups[doc.metadata['source_id']].append(i)\n",
    "    return groups\n",
    "\n",
    "def get_avg_intra_doc_similarity(docs, embeddings):\n",
    "    groups = group_by_source(docs)\n",
    "    result = {}\n",
    "    for source_id, indices in groups.items():\n",
    "        if len(indices) < 2:\n",
    "            continue\n",
    "        sims = []\n",
    "        for i in range(len(indices)):\n",
    "            for j in range(i + 1, len(indices)):\n",
    "                sim = cosine_similarity([embeddings[indices[i]]], [embeddings[indices[j]]])[0][0]\n",
    "                sims.append(sim)\n",
    "        result[source_id] = np.mean(sims)\n",
    "    return result\n",
    "\n",
    "def get_avg_inter_doc_similarity(docs, embeddings):\n",
    "    groups = group_by_source(docs)\n",
    "    doc_ids = list(groups.keys())\n",
    "    sims = []\n",
    "    for i in range(len(doc_ids)):\n",
    "        for j in range(i + 1, len(doc_ids)):\n",
    "            idx1 = groups[doc_ids[i]][0]\n",
    "            idx2 = groups[doc_ids[j]][0]\n",
    "            sim = cosine_similarity([embeddings[idx1]], [embeddings[idx2]])[0][0]\n",
    "            sims.append(sim)\n",
    "    return np.mean(sims)\n",
    "\n",
    "# ê³„ì‚°\n",
    "intra1 = get_avg_intra_doc_similarity(chunked_docs, embed_documents)\n",
    "inter1 = get_avg_inter_doc_similarity(chunked_docs, embed_documents)\n",
    "\n",
    "print(\"âœ… ë¬¸ì„œ ë‚´ë¶€ ìœ ì‚¬ë„ (intra-document avg)\")\n",
    "print(f\"ë°©ì‹2 í‰ê· : {np.mean(list(intra1.values())):.4f}\")\n",
    "# print(f\"ë°©ì‹1 í‰ê· : {np.mean(list(intra2.values())):.4f}\")\n",
    "\n",
    "print(\"\\nâœ… ë¬¸ì„œ ê°„ ìœ ì‚¬ë„ (inter-document avg)\")\n",
    "print(f\"ë°©ì‹2 í‰ê· : {inter1:.4f}\")\n",
    "# print(f\"ë°©ì‹1 í‰ê· : {inter2:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecbdef6",
   "metadata": {},
   "source": [
    "## ê²°ê³¼\n",
    "âœ… ë¬¸ì„œ ë‚´ë¶€ ìœ ì‚¬ë„ (intra-document avg)\n",
    "ì²­í¬ ë°©ì‹1 í‰ê· : 0.4946\n",
    "ì²­í¬ ë°©ì‹2 í‰ê· : 0.7137\n",
    "\n",
    "âœ… ë¬¸ì„œ ê°„ ìœ ì‚¬ë„ (inter-document avg)\n",
    "ì²­í¬ ë°©ì‹1 í‰ê· : 0.4169\n",
    "ì²­í¬ ë°©ì‹2 í‰ê· : 0.5358\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955bf922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ jnu-notice ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ì‚­ì œ í›„ ì¬ìƒì„±í•©ë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:12<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… 100ê°œ ì—…ë¡œë“œë¨\n",
      "âœ… 200ê°œ ì—…ë¡œë“œë¨\n",
      "âœ… 300ê°œ ì—…ë¡œë“œë¨\n",
      "âœ… 346ê°œ ì—…ë¡œë“œë¨\n",
      "âœ… Pinecone ì—…ë¡œë“œ ì™„ë£Œ! <pinecone.data.index.Index object at 0x3145dba90>\n"
     ]
    }
   ],
   "source": [
    "# Pinecone ë””ë¹„ ì—…ë¡œë“œ\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Pinecone í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "embedding_model = SentenceTransformer(\"snunlp/KR-SBERT-V40K-klueNLI-augSTS\")\n",
    "\n",
    "\n",
    "# ì²­í‚¹í•œ ë°ì´í„° ì„ë² ë”© í›„ Pinecone ì—…ë¡œë“œ\n",
    "def upload_documents(docs, index_name=INDEX_NAME):\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    index = pc.Index(index_name)\n",
    "    \n",
    "    if index_name in pc.list_indexes().names():\n",
    "        print(f\"âš ï¸ {index_name} ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ì‚­ì œ í›„ ì¬ìƒì„±í•©ë‹ˆë‹¤.\")\n",
    "        pc.delete_index(index_name)\n",
    "\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=768,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "    texts = [doc.page_content for doc in docs]\n",
    "    \n",
    "    # ì„ë² ë”©\n",
    "    embeddings = embedding_model.encode(texts, show_progress_bar=True)\n",
    "\n",
    "    # ì„ë² ë“œ ëœ ì •ë³´ì™€ í•´ë‹¹ ë‚´ìš© ê°™ì´ ì—…ë¡œë“œ\n",
    "    vectors = [\n",
    "        (f\"id_{i}\", emb.tolist(), {\"text\": texts[i]})\n",
    "        for i, emb in enumerate(embeddings)\n",
    "    ]\n",
    "\n",
    "    for i in range(0, len(vectors), 100):\n",
    "        index.upsert(vectors=vectors[i:i+100])\n",
    "        print(f\"âœ… {i+len(vectors[i:i+100])}ê°œ ì—…ë¡œë“œë¨\")\n",
    "\n",
    "    return index\n",
    "\n",
    "uploaded_index = upload_documents(chunked_docs, index_name=INDEX_NAME)\n",
    "\n",
    "print(\"âœ… Pinecone ì—…ë¡œë“œ ì™„ë£Œ!\",uploaded_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc40d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“¢ Gemini ì‘ë‹µ:\n",
      "ì•ˆë…•í•˜ì„¸ìš”! íœ´í•™ì„ ì›í•˜ì‹œëŠ”êµ°ìš”.  ğŸ˜Š\n",
      "\n",
      "2025í•™ë…„ë„ 1í•™ê¸° íœ´í•™ ì‹ ì²­ ê¸°ê°„ì€  íœ´í•™ ì¢…ë¥˜ì— ë”°ë¼ ë‹¤ë¥´ë„¤ìš”.  ìì„¸í•œ ë‚´ìš©ì€ ì•„ë˜ì™€ ê°™ì•„ìš”.\n",
      "\n",
      "* **ì¼ë°˜íœ´í•™(ë“±ë¡):** 2025ë…„ 2ì›” 18ì¼(í™”) ~ 4ì›” 24ì¼(ëª©)  (ë“±ë¡ê¸ˆ ë‚©ë¶€ í›„ íœ´í•™ì„ ì›í•˜ëŠ” ê²½ìš°)\n",
      "* **ì¼ë°˜íœ´í•™(ë¯¸ë“±ë¡):** 2025ë…„ 2ì›” 18ì¼(í™”) ~ 2ì›” 21ì¼(ê¸ˆ) (ë“±ë¡ê¸ˆ ë‚©ë¶€ ì „ íœ´í•™ì„ ì›í•˜ëŠ” ê²½ìš°)\n",
      "* **êµ°íœ´í•™(ë³µë¬´ì‹ ê³ ):** ì…ì˜ì¼ í•œ ë‹¬ ì „ ~ ì…ì˜ì¼ (í•™ê¸° ì¤‘ êµ°ì…ëŒ€ ì˜ˆì •ì¸ ê²½ìš°)\n",
      "* **ì§ˆë³‘íœ´í•™:** ì‚¬ìœ  ë°œìƒ ì‹œ ~ ì¢…ê°•ì¼ (ì§„ë‹¨ì„œ í•„ìš”)\n",
      "* **ì„ì‹ Â·ì¶œì‚°Â·ìœ¡ì•„íœ´í•™:** ì‚¬ìœ  ë°œìƒ ì‹œ ~ ì¢…ê°•ì¼ (ê´€ë ¨ ì„œë¥˜ í•„ìš”)\n",
      "* **ì°½ì—…íœ´í•™(ë“±ë¡):** 2025ë…„ 2ì›” 18ì¼(í™”) ~ 4ì›” 24ì¼(ëª©)\n",
      "\n",
      "ìì‹ ì˜ ìƒí™©ì— ë§ëŠ” íœ´í•™ ì¢…ë¥˜ë¥¼ í™•ì¸í•˜ì‹œê³ , í•´ë‹¹ ê¸°ê°„ ë‚´ì— ì‹ ì²­í•˜ì‹œëŠ” ê²ƒì„ ìŠì§€ ë§ˆì„¸ìš”!  í˜¹ì‹œ ë‹¤ë¥¸ ê¶ê¸ˆí•œ ì ì´ ìˆë‹¤ë©´ ë‹¨ê³¼ëŒ€í•™ í–‰ì •ì‹¤ì´ë‚˜ í•™ê³¼(ë¶€)ì‹¤ì— ë¬¸ì˜í•˜ì‹œë©´ ì¹œì ˆí•˜ê²Œ ì•ˆë‚´ë°›ìœ¼ì‹¤ ìˆ˜ ìˆì„ ê±°ì˜ˆìš”.  ğŸ˜„\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from tqdm import tqdm\n",
    "# ì–¸ì–´ëª¨ë¸ ì—°ê²°\n",
    "\n",
    "# Gemini ì„¤ì •\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "gemini_model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "\n",
    "# Gemini ì§ˆë¬¸í•˜ê¸°\n",
    "def query_with_gemini(index, query, top_k=10):\n",
    "    try:\n",
    "        # ì„ë² ë”©\n",
    "        query_embedding = embedding_model.encode(query).tolist()\n",
    "        \n",
    "        # Pinecone ê²€ìƒ‰\n",
    "        search_results = index.query(\n",
    "            vector=query_embedding,\n",
    "            top_k=top_k,\n",
    "            include_metadata=True\n",
    "        )\n",
    "\n",
    "        matches = search_results.get(\"matches\", [])\n",
    "        if not matches:\n",
    "            return \"ğŸ” ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì´ ë„ˆë¬´ êµ¬ì²´ì ì´ê±°ë‚˜ ê´€ë ¨ ë¬¸ì„œê°€ ì—†ì„ ìˆ˜ ìˆì–´ìš”.\"\n",
    "        # ê²€ìƒ‰ëœ ë¬¸ì„œ ì¶”ì¶œ\n",
    "        retrieved_chunks = [\n",
    "            match['metadata']['text']\n",
    "            for match in matches\n",
    "            if match.get('metadata') and match['metadata'].get('text')\n",
    "        ]\n",
    "        \n",
    "        # ìµœëŒ€ ê¸¸ì´ ì œí•œ (Gemini ì…ë ¥ ê¸¸ì´ ë°©ì§€)\n",
    "        context_text = \"\\n\\n\".join(retrieved_chunks)[:10000]\n",
    "\n",
    "        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "        prompt = f\"\"\"\n",
    "ğŸ” [ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½]:\n",
    "ë‹¤ìŒì€ '{query}'ì— ëŒ€í•´ ê²€ìƒ‰ëœ ë¬¸ì„œ ì¡°ê°ë“¤ì…ë‹ˆë‹¤.\n",
    "\n",
    "{context_text}\n",
    "\n",
    "ğŸ§  [ë‹¹ì‹ ì˜ ì—­í• ]:\n",
    "- ë‹¹ì‹ ì€ ì§€ì‹ ê¸°ë°˜ì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì‹ ë¢°ì„± ìˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n",
    "\n",
    "ğŸ“Œ [ë‹µë³€ ì§€ì¹¨]:\n",
    "1. ë¬¸ì„œì— ê´€ë ¨ ë‚´ìš©ì´ **ëª…í™•íˆ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´**, ê·¸ ë‚´ìš©ì„ ì¸ìš©í•´ ë‹µë³€í•˜ì„¸ìš”.\n",
    "2. **ì§ì ‘ì ì¸ ì •ë³´ê°€ ì—†ë”ë¼ë„**, ë¬¸ì„œ ë§¥ë½ì„ ë¶„ì„í•˜ì—¬ ê°€ëŠ¥í•œ **ì¶”ë¡ /í•´ì„**ì„ ì‹œë„í•´ ì£¼ì„¸ìš”.\n",
    "3. ë‹µë³€ì€ ì •í™•í•˜ë˜, **ì¹œì ˆí•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´**ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\n",
    "4. ë‹µë³€ê³¼ ì „í˜€ ê´€ë ¨ ì—†ëŠ” ë‚´ìš©ì€ ìƒëµí•´ì£¼ì„¸ìš”\n",
    "5. \"ì œê³µëœ ìë£Œì—ëŠ”*, ë¬¸ì„œì—ëŠ”* \" ìœ¼ë¡œ ì‹œì‘í•˜ì§€ ë§ì•„ì¤˜.\n",
    "6. ë„ˆë¬´ ë”±ë”±í•˜ì§€ ì•Šê²Œ , ì¹œì ˆí•˜ê³  ëŒ€í™”í•˜ë“¯ \"*ìš”\"ë¡œ ë§ˆë¬´ë¦¬ í•´ì¤˜.\n",
    "7. ê°€ë…ì„±ì´ ì¢‹ê²Œ ê° ë¬¸ì¥ë“¤ì„ ì¤„ë°”ê¿ˆ í•´ì¤˜. \n",
    "8. ë¬¸ì„œì— ì ì ˆí•œ ì´ëª¨í‹°ì½˜ì„ ë„£ì–´ ì½ê¸° ì‰½ê²Œ ë§Œë“¤ì–´ì¤˜.\n",
    "\n",
    "â“ [ì§ˆë¬¸]:\n",
    "{query}\n",
    "\n",
    "ğŸ“ [ë‹µë³€]:\n",
    "\"\"\"\n",
    "        # Gemini ì‘ë‹µ ìƒì„±\n",
    "        response = gemini_model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.types.GenerationConfig(\n",
    "                candidate_count=1,\n",
    "                temperature=0.8\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return response.text.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\"\n",
    "\n",
    "\n",
    "# âœ… ì‹¤ì œ ì‹¤í–‰\n",
    "# query = \"ì·¨ì—… ê´€ë ¨ ê³µê³ ê°€ ìˆë‚˜?\"\n",
    "# query = \"ì¡¸ì—… ìœ ë³´ë¥¼ í•˜ê³ ì‹¶ì€ë°, ìœ ë³´ì˜ ì¡°ê±´\"\n",
    "query = \"íœ´í•™ì„ í•˜ê³  ì‹¶ì–´. íœ´í•™ ì‹ ì²­ ê¸°ê°„ ì•Œë ¤ì¤˜.\"\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "index = pc.Index(INDEX_NAME)\n",
    "answer = query_with_gemini(uploaded_index, query)\n",
    "\n",
    "print(\"\\nğŸ“¢ Gemini ì‘ë‹µ:\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1302caa5",
   "metadata": {},
   "source": [
    "# í”¼ë“œë°±\n",
    "ìµœì¢… ë°œí‘œê¹Œì§€ ì´í™”ì—¬ëŒ€ ì±—ë´‡(íƒ€ê²Ÿì„œë¹„ìŠ¤) ì°¨ë³„ì„± êµ¬ì²´í™”í•´ì˜¤ê¸° + í˜„ì¬ ì´í™”ì—¬ëŒ€ ì±—ë´‡ì´ ì—…ë°ì´íŠ¸ê°€ ì•ˆë˜ê³  ìˆëŠ”ë° ì¹œêµ¬í•œí…Œ ì‹¤ì œ ì´í™”ì—¬ëŒ€ í•™ìƒë“¤ì´ ì‚¬ìš©í•˜ê³ ìˆëŠ”ì§€ ë¬¼ì–´ë³´ê¸°"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
