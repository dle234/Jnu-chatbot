# ì „ë‚¨ëŒ€í•™êµ ê³µì§€ì‚¬í•­ í¬ë¡¤ëŸ¬
import re
import time
import requests
from io import BytesIO
from PIL import Image
import pytesseract
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import os
from PIL import Image

# pytesseract.pytesseract.tesseract_cmd = '/opt/homebrew/bin/tesseract'
# os.environ['TESSDATA_PREFIX'] = '/opt/homebrew/share/tessdata'
chrome_options = Options()
chrome_options.add_argument("--headless") 
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")
driver = webdriver.Chrome(options=chrome_options)
os.environ["TOKENIZERS_PARALLELISM"] = "false"
# ìœ íš¨í•œ ì¹´í…Œê³ ë¦¬ ì •ì˜
valid_categories = ["í•™ì‚¬ì•ˆë‚´", "ëŒ€í•™ìƒí™œ", "ëª¨ì§‘ê³µê³ ", "ê³µëª¨ì „", "ì±„ìš©ê³µê³ ", "ì·¨ì—…ì •ë³´", "ì¥í•™ì•ˆë‚´", "ë³‘ë¬´ì•ˆë‚´"]

# í¬ë¡¤ë§ëœ ë°ì´í„° ì €ì¥ ë¦¬ìŠ¤íŠ¸
data = []

# í¬í„¸ ê¸°ë³¸ ì£¼ì†Œ
base_url = "https://www.jnu.ac.kr"

# í˜ì´ì§€ ë²”ìœ„ ì„¤ì •
for page in range(1, 2):
    print(f"ğŸ“„ [í˜ì´ì§€ {page}] í¬ë¡¤ë§ ì¤‘...")
    url = f"https://www.jnu.ac.kr/WebApp/web/HOM/COM/Board/board.aspx?boardID=5&bbsMode=list&cate=0&page={page}"
    driver.get(url)
    time.sleep(2)
    
    # ì „ë‚¨ëŒ€ í™ˆí˜ì´ì§€ ê³µì§€ì‚¬í•­ì—ì„œ ì œëª© íƒœê·¸
    notice_elements = driver.find_elements(By.CSS_SELECTOR, "td.title > a")
    
    # í™ˆí˜ì´ì§€ ì œëª©ìˆ˜ì§‘
    notices_to_process = []
    for el in notice_elements:
        try:
            full_title = el.text.strip()
            relative_link = el.get_attribute("href")
            link = relative_link 
            match = re.match(r"\[(.*?)\]\s*(.*)", full_title)
            # ì¹´í…Œê³ ë¦¬ ë³„ ë¶„ë¥˜
            if match:
                category = match.group(1)
                title = match.group(2)
            else:
                category = "ê¸°íƒ€"
                title = full_title
                
            if category not in valid_categories:
                category = "ê¸°íƒ€"
                
            notices_to_process.append({
                "category": category,
                "title": title,
                "link": link
            })
        except Exception as e:
            print(f"âš ï¸ ì œëª©/ë§í¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    
    # ìˆ˜ì§‘ëœ ë§í¬ë¥¼ í•˜ë‚˜ì”© ë°©ë¬¸í•˜ì—¬ ë‚´ìš© í¬ë¡¤ë§
    for notice in notices_to_process:
        try:
            print(f"ğŸ” ì²˜ë¦¬ ì¤‘: [{notice['category']}] {notice['title']}")
            driver.get(notice['link'])
            time.sleep(2)
            
            # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
            try:
                text_content = driver.find_element(By.CLASS_NAME, "view_body").text.strip()
                text_content = text_content.replace("\n", " ").replace("\r", " ")
            except Exception as e:
                print(f"âš ï¸ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                text_content = ""
            
            # ì´ë¯¸ì§€ OCR í…ìŠ¤íŠ¸ ìˆ˜ì§‘
            ocr_text = ""
            try:
                images = driver.find_elements(By.CSS_SELECTOR, ".view_body img")
                for img in images:
                    img_url = img.get_attribute("src")
                    if not img_url.startswith("http"):
                        img_url = base_url + img_url
                    
                    try:
                        response = requests.get(img_url)
                        image = Image.open(BytesIO(response.content))
                        
                        ocr_text += pytesseract.image_to_string(image, lang="kor") + "\n"
                    except Exception as e:
                        print(f"âš ï¸ ì´ë¯¸ì§€ OCR ì‹¤íŒ¨: {e}")
            except Exception as e:
                print(f"âš ï¸ ì´ë¯¸ì§€ ìš”ì†Œ ì°¾ê¸° ì‹¤íŒ¨: {e}")
            
            # ìˆ˜ì§‘í•œ ë‚´ìš© ì €ì¥
            content = text_content + "\n" + ocr_text.strip()
            notice["content"] = content
            data.append(notice)
            
        except Exception as e:
            print(f"âš ï¸ ê³µì§€ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
driver.quit()

# âœ… ê²°ê³¼ í™•ì¸ 
print(f"\nâœ… ì´ {len(data)}ê°œì˜ ê³µì§€ ìˆ˜ì§‘ ì™„ë£Œ!\n")
for notice in data[:3]:
    print(f"[{notice['category']}] {notice['title']}")
    print(f"ğŸ”— {notice['link']}")
    print(notice['content'][:100] + "..." if notice['content'] else "ë‚´ìš© ì—†ìŒ", "\n")
    
    
    
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from sentence_transformers import SentenceTransformer
import pandas as pd
import json

# langchainì„ í™œìš©í•˜ì—¬ ë°ì´í„° ì²­í‚¹ 
def chunk_documents(data, chunk_size=150, chunk_overlap=25):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        separators=["\n\n", "\n", " ", ""]
    )

    chunked_documents = []

    for item in data:
        metadata = {
            "category": item.get("category", ""),
            "title": item.get("title", ""),
            "link": item.get("link", ""),
            "source_id": item.get("id", "") if "id" in item else f"{item.get('title', '')[:20]}"
        }

        title = item.get("title", "")
        category = item.get("category", "")
        content = item.get("content", "")
        link = item.get("link", "")

        if content:
            content_chunks = text_splitter.split_text(content)

            for i, chunk in enumerate(content_chunks):
                # ë°©ì‹ 1ë²ˆ : full_chunk = chunk
                # ë°©ì‹ 2ë²ˆ : category + title + chunk ì¡°í•©
                full_chunk = f"[{category}] {title} - {chunk}"
                # full_chunk += f" (ì¶œì²˜: {link})"
                
                doc = Document(
                    page_content=full_chunk,
                    metadata={
                        **metadata,
                        "chunk_id": i,
                        "chunk_count": len(content_chunks)
                    }
                )
                
                chunked_documents.append(doc)
        else:
            full_chunk = f"[{category}] {title}"
            doc = Document(
                page_content=full_chunk,
                metadata=metadata
            )
            chunked_documents.append(doc)

    return chunked_documents



# í•œêµ­ì–´ ì „ìš© SBERT ëª¨ë¸ ë¡œë”©
embedding_model = SentenceTransformer('snunlp/KR-SBERT-V40K-klueNLI-augSTS')

# ì˜ˆì‹œ ì„ë² ë”© í•¨ìˆ˜
def embed_documents(docs):
    texts = [doc.page_content for doc in docs]
    embeddings = embedding_model.encode(texts, show_progress_bar=True)
    return embeddings

chunked_docs = chunk_documents(data)
embed_documents(chunked_docs)

# Pinecone ë””ë¹„ ì—…ë¡œë“œ

from pinecone import Pinecone, ServerlessSpec
import numpy as np
import json

# Pinecone í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
embedding_model = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")
INDEX_NAME = "jnu-notice"

# ì²­í‚¹í•œ ë°ì´í„° ì„ë² ë”© í›„ Pinecone ì—…ë¡œë“œ
def upload_documents(docs, index_name=INDEX_NAME):
    pc = Pinecone(api_key=PINECONE_API_KEY)
    index = pc.Index(index_name)
    
    if index_name in pc.list_indexes().names():
        print(f"âš ï¸ {index_name} ì¸ë±ìŠ¤ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ì‚­ì œ í›„ ì¬ìƒì„±í•©ë‹ˆë‹¤.")
        pc.delete_index(index_name)

    pc.create_index(
        name=index_name,
        dimension=768,
        metric="cosine",
        spec=ServerlessSpec(cloud="aws", region="us-east-1")
    )

    texts = [doc.page_content for doc in docs]
    
    # ì„ë² ë”©
    embeddings = embedding_model.encode(texts, show_progress_bar=True)

    # ì„ë² ë“œ ëœ ì •ë³´ì™€ í•´ë‹¹ ë‚´ìš© ê°™ì´ ì—…ë¡œë“œ
    vectors = [
        (f"id_{i}", emb.tolist(), {"text": texts[i]})
        for i, emb in enumerate(embeddings)
    ]

    for i in range(0, len(vectors), 100):
        index.upsert(vectors=vectors[i:i+100])
        print(f"âœ… {i+len(vectors[i:i+100])}ê°œ ì—…ë¡œë“œë¨")

    return index

uploaded_index = upload_documents(chunked_docs, index_name=INDEX_NAME)

print("âœ… Pinecone ì—…ë¡œë“œ ì™„ë£Œ!",uploaded_index)

import google.generativeai as genai
from tqdm import tqdm
# ì–¸ì–´ëª¨ë¸ ì—°ê²°

import streamlit as st

# Streamlit secretsì—ì„œ API í‚¤ ë¶ˆëŸ¬ì˜¤ê¸°
GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
PINECONE_API_KEY = st.secrets["PINECONE_API_KEY"]

# Gemini ì„¤ì •
genai.configure(api_key=GOOGLE_API_KEY)
gemini_model = genai.GenerativeModel("gemini-1.5-flash")


# Gemini ì§ˆë¬¸í•˜ê¸°
def query_with_gemini(index, query, top_k=10):
    try:
        # ì„ë² ë”©
        query_embedding = embedding_model.encode(query).tolist()
        
        # Pinecone ê²€ìƒ‰
        search_results = index.query(
            vector=query_embedding,
            top_k=top_k,
            include_metadata=True
        )

        matches = search_results.get("matches", [])
        if not matches:
            return "ğŸ” ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì´ ë„ˆë¬´ êµ¬ì²´ì ì´ê±°ë‚˜ ê´€ë ¨ ë¬¸ì„œê°€ ì—†ì„ ìˆ˜ ìˆì–´ìš”."
        # ê²€ìƒ‰ëœ ë¬¸ì„œ ì¶”ì¶œ
        retrieved_chunks = [
            match['metadata']['text']
            for match in matches
            if match.get('metadata') and match['metadata'].get('text')
        ]
        
        # ìµœëŒ€ ê¸¸ì´ ì œí•œ (Gemini ì…ë ¥ ê¸¸ì´ ë°©ì§€)
        context_text = "\n\n".join(retrieved_chunks)[:10000]

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""
ğŸ” [ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½]:
ë‹¤ìŒì€ '{query}'ì— ëŒ€í•´ ê²€ìƒ‰ëœ ë¬¸ì„œ ì¡°ê°ë“¤ì…ë‹ˆë‹¤.

{context_text}

ğŸ§  [ë‹¹ì‹ ì˜ ì—­í• ]:
- ë‹¹ì‹ ì€ ì§€ì‹ ê¸°ë°˜ì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì‹ ë¢°ì„± ìˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ğŸ“Œ [ë‹µë³€ ì§€ì¹¨]:
1. ë¬¸ì„œì— ê´€ë ¨ ë‚´ìš©ì´ **ëª…í™•íˆ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´**, ê·¸ ë‚´ìš©ì„ ì¸ìš©í•´ ë‹µë³€í•˜ì„¸ìš”.
2. **ì§ì ‘ì ì¸ ì •ë³´ê°€ ì—†ë”ë¼ë„**, ë¬¸ì„œ ë§¥ë½ì„ ë¶„ì„í•˜ì—¬ ê°€ëŠ¥í•œ **ì¶”ë¡ /í•´ì„**ì„ ì‹œë„í•´ ì£¼ì„¸ìš”.
3. ë‹µë³€ì€ ì •í™•í•˜ë˜, **ì¹œì ˆí•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´**ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
4. ë‹µë³€ê³¼ ì „í˜€ ê´€ë ¨ ì—†ëŠ” ë‚´ìš©ì€ ìƒëµí•´ì£¼ì„¸ìš”
5. "ì œê³µëœ ìë£Œì—ëŠ”*, ë¬¸ì„œì—ëŠ”* " ìœ¼ë¡œ ì‹œì‘í•˜ì§€ ë§ì•„ì¤˜.
6. ë„ˆë¬´ ë”±ë”±í•˜ì§€ ì•Šê²Œ , ì¹œì ˆí•˜ê³  ëŒ€í™”í•˜ë“¯ "*ìš”"ë¡œ ë§ˆë¬´ë¦¬ í•´ì¤˜.
7. ê°€ë…ì„±ì´ ì¢‹ê²Œ ê° ë¬¸ì¥ë“¤ì„ ì¤„ë°”ê¿ˆ í•´ì¤˜. 
8. ë¬¸ì„œì— ì ì ˆí•œ ì´ëª¨í‹°ì½˜ì„ ë„£ì–´ ì½ê¸° ì‰½ê²Œ ë§Œë“¤ì–´ì¤˜.

â“ [ì§ˆë¬¸]:
{query}

ğŸ“ [ë‹µë³€]:
"""
        # Gemini ì‘ë‹µ ìƒì„±
        response = gemini_model.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
                candidate_count=1,
                temperature=0.8
            )
        )

        return response.text.strip()
    
    except Exception as e:
        return f"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


# âœ… ì‹¤ì œ ì‹¤í–‰
# query = "ì·¨ì—… ê´€ë ¨ ê³µê³ ê°€ ìˆë‚˜?"
# query = "ì¡¸ì—… ìœ ë³´ë¥¼ í•˜ê³ ì‹¶ì€ë°, ìœ ë³´ì˜ ì¡°ê±´"
query = "íœ´í•™ì„ í•˜ê³  ì‹¶ì–´. íœ´í•™ ì‹ ì²­ ê¸°ê°„ ì•Œë ¤ì¤˜."

pc = Pinecone(api_key=PINECONE_API_KEY)
index = pc.Index(INDEX_NAME)
answer = query_with_gemini(uploaded_index, query)

print("\nğŸ“¢ Gemini ì‘ë‹µ:")
print(answer)
